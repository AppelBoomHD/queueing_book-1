% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: pythontex: {verbose: yes, rerun: modified }
% arara: pdflatex: { shell: yes, interaction: nonstopmode }
% arara: clean: { extensions: [ aux, blg, idx, ilg, ind, log, out, pytxcode, rel, toc ] }
% !arara: clean: { files: [ ans.tex, hint.tex] }

\input{header}

\section{Poisson Arrivals See Time Averages}
\label{sec:poisson-arrivals-see}

Suppose jobs arrive exactly at the start of an hour and require 59 minutes of service.
If we sample the server occupation at job arrival times, the server is always free, while if we track the server over time, it is nearly always busy.
Thus, what jobs see upon arrival is \emph{in general not equal} to what the server perceives.
However, when jobs arrive as a Poisson process, both sampling methods produce the same number, a result known as the \recall{Poisson arrivals see time averages} (\recall{PASTA}) property. Here we will discuss this property in more detail.



Recall from~\cref{sec:level-cross-balance} that
\begin{subequations}\label{eq:10444}
\begin{equation}
\frac{A(n,t)}t = \frac{A(n,t)}{Y(n,t)}\frac{Y(n,t)}t \to \lambda(n) p(n), \quad\text{as } t \to \infty.
\end{equation}
Rather than multiplying and dividing the LHS by $Y(n,t)$, we can also multiply and divide by $A(t)$ to get another interesting result:
\begin{equation}\label{eq:1333}
 \frac{A(n,t)}{t} 
= \frac{A(t)}t \frac{A(n,t)}{A(t)}.
%\to \lambda \pi(n), \quad\text{as } t \to \infty.
\end{equation}
\end{subequations}
It is clear that $A(t)/t\to \lambda$, so let us interpret $A(n,t)/A(t)$.
For this, observe first
\begin{equation*}
\frac{A(n,t)}{A(t)} = 
\frac1{A(t)}\sum_{k=1}^\infty \1{A_k \leq t, L(A_k-) = n} =   \frac1{A(t)}\sum_{k=1}^{A(t)} \1{\L(A_k-) = n}.
\end{equation*}
Then, since $A(t)\to \infty$ as $t\to\infty$, it follows\sidenote{\cref{ex:18}} that 
\begin{equation*}
\lim_{t\to\infty}\frac1{A(t)}\sum_{k=1}^{A(t)} \1{\L(A_k-) = n} =  \lim_{m\to\infty} \frac1m\sum_{k=1}^m \1{\L(A_k-) = n} =: \pi(n),
\end{equation*}
where  $\pi(n)$ is the long-run fraction of arrivals that observe $n$ customers in the system.

But, in~\cref{eq:10444} the LHSs are equal. Hence, 
\begin{equation}\label{eq:13}
\lambda \pi(n) = \lambda(n) p(n),
\end{equation}
which implies the main result:
\begin{equation*}
 \lambda(n) = \lambda \iff \pi(n) = p(n).
\end{equation*}
In words,  if the arrival rate does not depend on the state of the system, i.e., $\lambda(n)=\lambda$, the sample probabilities $\{\pi(n)\}$ are equal to the time-average probabilities $\{p(n)\}$.\sidenote{Thus, what arrivals see agrees with what the  server sees.}

As the above example showed, this property is not satisfied in general.
However, when the arrival process is Poisson we have\sidenote{A rigorous proof of this is hard, see e.g., \cite{el-taha98:_sampl_path_analy_queuein_system}} that $\lambda(n)=\lambda$, and its implication $\pi(n)=p(n)$ is known as the \emph{PASTA} property.


\newthought{With similar reasoning}, we can also establish a relation between $\pi(n)$ and $\delta(n)$, i.e.,  statistics as obtained by the departures.
Define, analogous to $\pi(n)$, 
\begin{equation*}
 \delta(n) = \lim_{t\to\infty} \frac{D(n,t)}{D(t)}
\end{equation*}
as the long-run fraction of jobs that leave $n$ jobs \emph{behind}.
From~\cref{eq:15},
\begin{equation*}
\frac{A(t)}t \frac{A(n,t)}{A(t)} = \frac{A(n,t)}t \approx \frac{D(n,t)}t 
= \frac{D(t)}t \frac{D(n,t)}{D(t)}.
\end{equation*}
Taking limits at the left and right, and using~\cref{eq:28}, we obtain for the $G/G/c$ queue\sidenote{Because  customers arrive and leave as single units in a $G/G/c$ queue.}
\begin{equation} \label{eq:36}
 \lambda \pi(n) = \delta \delta(n).
\end{equation}
Consequently, for the  rate-stable $G/G/c$ queue  the statistics obtained by arrivals is the same as statistics obtained by departures, i.e., 
\begin{equation} \label{eq:39}
\lambda = \delta \iff \pi(n) = \delta(n).
\end{equation}

\begin{exercise}\label{ex:8} 
Show\marginpar{Continuation of~\cref{ex:111}} that $\pi(0)=1$ and $\pi(n)=0$, for $n>0$.
\begin{solution}
  All arrivals see an empty system.
  Hence, $A(0,t)/A(t) \approx (t/2)/(t/2) = 1$, and $A(n,t)=0$ for $n>0$.
  Thus, $\pi(0) = \lim_{t\to\infty} A(0,t)/A(t) = 1$ and $\pi(n)=0$ for $n>0$.
  Recall from the other exercises that $p(0)=1/2$.
  Hence, statistics as obtained via time averages are not necessarily the same as statistics obtained at arrival moments (or any other point process).
\end{solution}

\end{exercise}

\begin{exercise}\label{ex:l-152}
 Check\marginpar{Continuation of~\cref{ex:8}} that~\cref{eq:13} holds.
\begin{solution}
From the relevant previous exercises, $\lambda = \lim_{t\to\infty} A(t)/t = 1/2$. $\lambda(0)=1$, $p(0)=1/2$, and $\pi(0)=1$. Hence,
\begin{equation*}
 \lambda \pi(0) = \lambda(0) p(0) \implies \frac 1 2 \times 1 = 1\times \frac 1 2.
\end{equation*}
For $n>0$ it's easy, everything is 0.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:26}
 When $\lambda\neq \delta$, is $\pi(n)\geq \delta(n)$? 
\begin{hint}
 Use that $\lambda \geq \delta$ always holds. Thus, when $\lambda \neq \delta$, it must be that $\lambda > \delta$. What are the consequences of this inequality; how does the queue length behave as a function of time?
\end{hint}
\begin{solution}
 The assumptions lead us to conclude that $\lambda > \delta$. As a consequence, the queue length must increase in the long run (jobs come in faster than they leave). Therefore, $A(n,t)/t \to 0$ for all $n$, and also $D(n,t)/t\to 0$. Consequently, $\pi(n) = \delta(n) = 0$, which is the only sensible reconciliation with~\cref{eq:36}. 
\end{solution}
\end{exercise}



\begin{exercise}\label{ex:18}
Use the renewal-reward theorem to prove that $A(n,t)/t \to \lambda \pi(n)$.
\begin{hint}
Check that the conditions of the renewal reward theorem are satisfied in the above proof of~\cref{eq:1333}. Then define 
\begin{align*}
 Y(t) &:= A(n,t) = \sum_{k=1}^{A(t)} \1{\L(A_k-) = n} \\
X_k &:= Y(A_k) - Y(A_{k-1}) = A(n, A_k) - A(n, A_{k-1}) = \1{\L(A_k-)=n}.
\end{align*}

\end{hint}
\begin{solution}
First we check the conditions. The counting process here is $\{A(t)\}$ and the epochs at which
 $A(t)$ increases are $\{A_k\}$. By assumption, $A_k\to\infty$,
 hence $A(t)\to\infty$ as $t\to\infty$. Moreover, by assumption
 $A(t)/t \to \lambda$. Also $A(n,t)$ is evidently non-decreasing and
 $A(n,t)\to\infty$ as $t\to\infty$.


From the definitions in the hint, 
\begin{equation*}
X= \lim_{m\to\infty} \frac 1 m \sum_{k=1}^m X_k =\lim_{m\to\infty} \frac 1 m \sum_{k=1}^m \1{\L(A_k-)=n} = \pi(n).
\end{equation*}
Since $Y=\lim_{t\to\infty} Y(t)/t = \lim_{t\to\infty} A(n,t)/t$ it follows from the renewal reward theorem that
\begin{equation*}
 Y=\lambda X \implies \lim_{t\to\infty} \frac{A(n,t)} t = \lambda X = \lambda \pi(n).
\end{equation*}
\end{solution}
\end{exercise}


\input{trailer}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
